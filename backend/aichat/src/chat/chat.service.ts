import { Injectable, OnModuleInit } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import {
  BedrockRuntimeClient,
  ConverseCommand,
  ConverseCommandInput
} from '@aws-sdk/client-bedrock-runtime';
import { BedrockEmbeddings } from '@langchain/aws';
import { FaissStore } from '@langchain/community/vectorstores/faiss';
import { Document } from '@langchain/core/documents';
import * as fs from 'fs';

@Injectable()
export class ChatService implements OnModuleInit {
  private embeddings: BedrockEmbeddings;
  private vectorStore: FaissStore;
  private bedrockClient: BedrockRuntimeClient;
  private readonly VECTOR_STORE_PATH = './vector_store';

  constructor(private configService: ConfigService) {}

  async onModuleInit() {
    const accessKeyId = this.configService.get<string>('AWS_ACCESS_KEY_ID') ?? '';
    const secretAccessKey = this.configService.get<string>('AWS_SECRET_ACCESS_KEY') ?? '';
    const region = this.configService.get<string>('AWS_REGION') ?? 'us-east-1';

    // 1. ì„ë² ë”© ëª¨ë¸ (LangChain)
    this.embeddings = new BedrockEmbeddings({
      region: region,
      credentials: { accessKeyId, secretAccessKey },
      model: 'amazon.titan-embed-text-v2:0',
    });

    // 2. Bedrock SDK Client (Converse APIìš©)
    this.bedrockClient = new BedrockRuntimeClient({
      region: region,
      credentials: { accessKeyId, secretAccessKey },
    });

    await this.loadVectorStore();
  }

  private async loadVectorStore() {
    if (fs.existsSync(this.VECTOR_STORE_PATH)) {
      console.log('ğŸ“‚ Loading existing vector store...');
      this.vectorStore = await FaissStore.load(this.VECTOR_STORE_PATH, this.embeddings);
    } else {
      console.log('ğŸ†• Creating new vector store...');
      this.vectorStore = await FaissStore.fromDocuments(
        [new Document({ pageContent: 'Init Data', metadata: { source: 'init' } })],
        this.embeddings
      );
      await this.vectorStore.save(this.VECTOR_STORE_PATH);
    }
  }

  async addKnowledge(content: string, source: string) {
    const doc = new Document({ pageContent: content, metadata: { source } });
    await this.vectorStore.addDocuments([doc]);
    await this.vectorStore.save(this.VECTOR_STORE_PATH);
    return { message: 'Knowledge added.', source };
  }

  // [ê¸°ì¡´ ìœ ì§€] AI í…ìŠ¤íŠ¸ ê¸°ë°˜ ì°¨ì¢… ë¶„ë¥˜
  async classifyCar(modelName: string): Promise<string> {
    const prompt = `Classify '${modelName}' into ONE: [Sedan, SUV, Truck, Van, Light Car, Sports Car, Hatchback]. No explanation.`;
    const input: ConverseCommandInput = {
      modelId: 'us.meta.llama3-3-70b-instruct-v1:0',
      messages: [{ role: 'user', content: [{ text: prompt }] }],
      inferenceConfig: { maxTokens: 10, temperature: 0 },
    };
    try {
      const command = new ConverseCommand(input);
      const res = await this.bedrockClient.send(command);
      return res.output?.message?.content?.[0]?.text?.trim().split(/[\n,.]/)[0].trim() || 'ê¸°íƒ€';
    } catch (e) { return 'ê¸°íƒ€'; }
  }

  // =================================================================================
  // [ì‹ ê·œ ê¸°ëŠ¥] ì´ë¯¸ì§€ ì±„íŒ… (Llama 3.2 Vision + RAG Pipeline + CoT Reasoning)
  // =================================================================================

  async chatWithImage(imageBuffer: Buffer, mimeType: string = 'image/jpeg') {
    console.log("ğŸ“¸ Image received, analyzing with Llama 3.2 Vision...");

    // 1. Vision ëª¨ë¸ë¡œ ì°¨ì¢… ì‹ë³„
    const identifiedCarName = await this.identifyCarWithLlama(imageBuffer, mimeType);

    if (identifiedCarName === 'NOT_CAR') {
        return {
            response: "ì£„ì†¡í•©ë‹ˆë‹¤. ì‚¬ì§„ì—ì„œ ìë™ì°¨ë¥¼ ëª…í™•í•˜ê²Œ ì‹ë³„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì°¨ëŸ‰ì´ ì˜ ë³´ì´ëŠ” ì‚¬ì§„ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
            context_used: [],
            identified_car: null
        };
    }

    console.log(`ğŸ“¸ Identified Car: ${identifiedCarName}`);

    // 2. ì‹ë³„ëœ ì°¨ì¢…ìœ¼ë¡œ ë²¡í„° ìŠ¤í† ì–´ ê²€ìƒ‰ (RAG)
    const results = await this.vectorStore.similaritySearch(identifiedCarName, 10);
    const contextText = results.map(doc => doc.pageContent).join("\n");
    const sources = results.map((r) => r.metadata.source);

    // 3. ê²€ìƒ‰ëœ ì •ë³´(Context)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¤ëª… ìƒì„± (ë§í¬ í¬í•¨ ê¸°ëŠ¥ ì¶”ê°€ë¨)
    const description = await this.generateCarDescription(identifiedCarName, contextText);

    return {
        response: description,
        context_used: sources,
        identified_car: identifiedCarName
    };
  }

  // [Helper] ì‹ë³„ëœ ì •ë³´ë¡œ ì„¤ëª… ìƒì„± (Llama 3.3 70B ì‚¬ìš©) - ë§í¬ ë¡œì§ ì¶”ê°€ë¨
  private async generateCarDescription(carName: string, context: string): Promise<string> {
      const prompt = `
<|begin_of_text|><|start_header_id|>system<|end_header_id|>
You are an AI Automotive Expert at 'AlphaCar'.
An image uploaded by the user has been identified as **'${carName}'**.

Your goal is to explain this vehicle to the user based **ONLY** on the provided [Context] from our vector store.

[INSTRUCTIONS]
1. **Source of Truth**: You MUST answer based solely on the [Context]. Do not use external training data.
2. **Structure**:
   - **Introduction**: "ì—…ë¡œë“œí•˜ì‹  ì‚¬ì§„ì€ **${carName}**ì…ë‹ˆë‹¤."
   - **Image Display (CRITICAL)**: You MUST display the car image from the context.
   - **Key Features**: Summarize 3 key selling points.
   - **Specs**: Mention price range or fuel efficiency.
   - **Call to Action**: Encourage checking the detailed quote.
3. **Language**: Output in **Korean (Hangul)**.

[IMAGE RENDERING & LINKING LOGIC - STRICT]
- The user MUST be able to click the image to see the quote.
- **Step 1**: Find 'ì´ë¯¸ì§€URL' (or 'ImageURL') in the [Context].
- **Step 2**: Find 'BaseTrimId' in the [ì‹œìŠ¤í…œ ë°ì´í„°] section of the [Context].
- **Step 3**: Generate the image link using this EXACT Markdown format:
  
  [![${carName}](ì´ë¯¸ì§€URL_ê°’)](/quote/personal/result?trimId=BaseTrimId_ê°’)

- **WARNING**: Do NOT output raw URLs. Only use the Markdown link format above.

[Context (Vector Store Data)]
${context}

<|eot_id|><|start_header_id|>user<|end_header_id|>
ì´ ì°¨ì— ëŒ€í•´ ìš°ë¦¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìì„¸íˆ ì„¤ëª…í•´ì£¼ê³ , ê²¬ì ì„ ë³¼ ìˆ˜ ìˆê²Œ ì‚¬ì§„ì— ë§í¬ë¥¼ ê±¸ì–´ì¤˜.
<|eot_id|><|start_header_id|>assistant<|end_header_id|>
`;

      const input: ConverseCommandInput = {
        modelId: 'us.meta.llama3-3-70b-instruct-v1:0',
        messages: [{ role: 'user', content: [{ text: prompt }] }],
        inferenceConfig: { maxTokens: 2048, temperature: 0.2 },
      };

      try {
        const command = new ConverseCommand(input);
        const response = await this.bedrockClient.send(command);
        return response.output?.message?.content?.[0]?.text || 'ì°¨ëŸ‰ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.';
      } catch (e) {
        console.error("ğŸ”¥ Bedrock Description Gen Error:", e);
        return 'ì°¨ëŸ‰ ì„¤ëª… ìƒì„± ì‹¤íŒ¨';
      }
  }

  // [Helper] ì´ë¯¸ì§€ ì‹ë³„ (Llama 3.2 90B Vision)
  private async identifyCarWithLlama(imageBuffer: Buffer, mimeType: string): Promise<string> {
    const modelId = 'us.meta.llama3-2-90b-instruct-v1:0';

    const prompt = `
<|begin_of_text|><|start_header_id|>system<|end_header_id|>
You are an expert automotive visual recognition AI.
Your task is to identify the vehicle in the image with extreme precision.

[OUTPUT FORMAT]
Reasoning: [Reasoning in English]
Final Answer: [Manufacturer ModelName in Korean]

[EXAMPLES]
User:
Assistant:
Reasoning: I see the KN logo and sliding doors. It is a minivan.
Final Answer: ê¸°ì•„ ì¹´ë‹ˆë°œ

User:
Assistant:
Reasoning: This is a dog.
Final Answer: NOT_CAR
<|eot_id|><|start_header_id|>user<|end_header_id|>
Identify the car in this image.
<|eot_id|><|start_header_id|>assistant<|end_header_id|>
`;

    const format = mimeType === 'image/png' ? 'png' :
                   mimeType === 'image/webp' ? 'webp' :
                   mimeType === 'image/gif' ? 'gif' : 'jpeg';

    const input: ConverseCommandInput = {
      modelId: modelId,
      messages: [
        {
          role: 'user',
          content: [
            {
              image: {
                format: format as any,
                source: { bytes: imageBuffer },
              },
            },
            { text: prompt },
          ],
        },
      ],
      inferenceConfig: { maxTokens: 300, temperature: 0.1 },
    };

    try {
      const command = new ConverseCommand(input);
      const response = await this.bedrockClient.send(command);
      const fullText = response.output?.message?.content?.[0]?.text || '';
      console.log("ğŸ¤– Vision Thinking Process:", fullText);

      const match = fullText.match(/Final Answer:\s*(.*)/i);
      let identifiedName = 'NOT_CAR';
      if (match && match[1]) {
          identifiedName = match[1].trim();
      } else if (fullText.includes("NOT_CAR")) {
          identifiedName = "NOT_CAR";
      } else {
          identifiedName = fullText.replace(/Reasoning:[\s\S]*?Final Answer:/i, "").trim();
      }
      
      identifiedName = identifiedName.replace(/\.$/, '').trim();
      if (identifiedName.includes('NOT_CAR')) return 'NOT_CAR';
      return identifiedName;

    } catch (e) {
      console.error("ğŸ”¥ Bedrock Vision Error:", e);
      return 'NOT_CAR';
    }
  }

  // =================================================================================

  async chat(userMessage: string) {
    // 1. RAG ê²€ìƒ‰
    let results = await this.vectorStore.similaritySearch(userMessage, 20);

    const context = results.map((r) => r.pageContent).join('\n\n');
    const sources = results.map((r) => r.metadata.source);

    console.log(`ğŸ” Context Length: ${context.length} characters`);

    const comparisonKeywords = ['ë¹„êµ', 'ëŒ€ë¹„', 'ë­ê°€ ë”', 'ì°¨ì´'];
    const isComparisonQuery = comparisonKeywords.some(keyword => userMessage.includes(keyword)) &&
                              (userMessage.includes('ì˜ë‚˜íƒ€') && userMessage.includes('K5'));

    // 2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    let systemPrompt = `
    You are the AI Automotive Specialist for 'AlphaCar'.

    [CORE RULES]
    1. **LANGUAGE**: Answer strictly in **Korean (Hangul)**.
    2. **GROUNDING**: Answer SOLELY based on the provided [Context].
    3. **GUARDRAIL**: Reject non-automotive topics.

    [IMAGE RENDERING & LINKING LOGIC - CRITICAL]
    - If the context contains 'ImageURL' and 'BaseTrimId' for the suggested car, you **MUST** display the image wrapped in a link.
    - **Purpose**: Clicking the image should take the user to the quote page.
    - **STRICT Format**:
      [![Car Name](ImageURL)](/quote/personal/result?trimId=BaseTrimId)

    - **Instruction**:
      1. Extract 'ImageURL' from the context.
      2. Extract 'BaseTrimId' from the [ì‹œìŠ¤í…œ ë°ì´í„°] section.
      3. Combine them into the Markdown link above. Do NOT use placeholder IDs.

    [RESPONSE STRATEGY]
    - Act like a friendly, professional car dealer.
    - End with a follow-up question.

    ${isComparisonQuery ? `
    [COMPARISON MODE]
    - Output two distinct blocks for each car.
    - Start each block with the clickable image link (Format above).
    - Compare Price and Key Options.
    ` : ''}

    [Context]
    ${context}
    `;

    // 3. Bedrock Converse API
    const guardrailId = this.configService.get<string>('BEDROCK_GUARDRAIL_ID');
    const guardrailVersion = this.configService.get<string>('BEDROCK_GUARDRAIL_VERSION') || 'DRAFT';

    const input: ConverseCommandInput = {
      modelId: 'us.meta.llama3-3-70b-instruct-v1:0',
      messages: [{ role: 'user', content: [{ text: userMessage }] }],
      system: [{ text: systemPrompt }],
      inferenceConfig: { maxTokens: 2048, temperature: 0.2 },
    };

    if (guardrailId && guardrailId.length > 5) {
        input.guardrailConfig = {
            guardrailIdentifier: guardrailId,
            guardrailVersion: guardrailVersion,
            trace: 'enabled',
        };
    }

    try {
      const command = new ConverseCommand(input);
      const response = await this.bedrockClient.send(command);

      if (response.stopReason === 'guardrail_intervened') {
          return { response: "ğŸš« ì£„ì†¡í•©ë‹ˆë‹¤. ê·¸ ì§ˆë¬¸ì€ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", context_used: [] };
      }

      const outputText = response.output?.message?.content?.[0]?.text || '';
      return { response: outputText, context_used: sources };

    } catch (e: any) {
      console.error("ğŸ”¥ AWS Bedrock Error:", e.message);
      return { response: "ì£„ì†¡í•©ë‹ˆë‹¤. AI ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", context_used: [] };
    }
  }
}
